{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJiDbsmn_egZ"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29T9Sq_F_942",
        "outputId": "56ae8acf-f4e5-458b-f225-159b28a22df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VplP1EsCAAGk"
      },
      "source": [
        "# Overview\n",
        "\n",
        "In this project, we will use collaborative filtering using a matrix factorization algorithm called `Alternating Least Squares (ALS)` with Spark APIs to predict the ratings for the movies in [MovieLens Datasets](https://grouplens.org/datasets/movielens/latest/)\n",
        "\n",
        "[Alternating Least Squares](https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E6%8E%A8%E8%8D%90/papers/Large-scale%20Parallel%20Collaborative%20Filtering%20the%20Netflix%20Prize.pdf) is one of the low-rank matrix approximation algorithms for collaborative filtering. ALS decomposes the user-item matrix into two low-dimensional matrixes: user matrix and item matrix. In collaborative filtering, users and products are described by a small set of latent factors that can be used to predict missing entries. And ALS algorithm learns these latent factors by matrix factorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub9epS17AoEb"
      },
      "source": [
        "## Collaborative Filtering using ALS Matrix Factorization\n",
        "\n",
        "**Q: What is matrix factorization?**\n",
        "\n",
        "A: Matrix factorization is a set of algorithms that factor a matrix into the product of multiple matrices. \n",
        "\n",
        "**Q: Why is matrix factorization useful for recommender systems?**\n",
        "\n",
        "A: We can factor the items-users interaction matrix into the product of 2 lower dimensional rectangular matrices: items-matrix and user-matrix. Here each row of the items matrix can be seen as a vector representation of an item, and each column of the user matrix can be seen as a vector representation of a user.\n",
        "\n",
        "![matrix_factorization](https://raw.githubusercontent.com/wjlgatech/amazing_recommender/master/images/matrix_factorization.png)\n",
        "\n",
        "\n",
        "The elements of the item vector do not have a necessarily interpretable meaning by themselves, but the whole vector represents how that item's features interact with users. Therefore these features are called latent (e.g.hidden, or meaning unclear if you like) features.\n",
        "$$ \\hat{R}_{u,i} = \\sum_{f=1:n} U_{u,f} I_{f,i}$$\n",
        "\n",
        "$ \\hat{R}_{u,i}$ is the Rating of item `i` given by user `u` equals the dot product of `user latent vector` $U_u$ and `item latent vector` $I_i$.\n",
        "\n",
        "**Q: How do you choose the number of latent factors `n` in the above formula?**\n",
        "\n",
        "A: The number of latent factors is a tunable hyperparameter. It's very similar to PCA, where you can choose the number of PC, which affects how much data variance you can capture.\n",
        "\n",
        "When you increase the number of latent factors, personalization will increase until n becomes too big and the model starts to overfit. \n",
        "\n",
        "In order to prevent overfitting, we add **regularization terms** to the objective function.\n",
        "\n",
        "**Q: What's the final expression for the objective function?**\n",
        "\n",
        "The **objective function** of matrix factorization is designed to help minimize the error between true rating and predicted rating.\n",
        "\n",
        "$$arg min_{U,I} ||R - \\hat{R}||+\\alpha ||U|| + \\beta ||I||$$\n",
        "\n",
        "There are also regularization terms for the user factors ($\\alpha ||U||$) and for the item factors ($\\beta ||I||$).\n",
        "\n",
        "**Q: How is the optimization problem setup?**\n",
        "\n",
        "Once we have the objective function, we need to design an algorithm to solve the optimization problem.\n",
        "\n",
        "- In case the data is small enough can be fit into a single machine, you can use **Funk SVD**, an algo developed by Simon Funk during the 2006 Netflix Challenge. \n",
        "\n",
        "- In case the data is too big to fit into a single machine, you can use Alternating Least Square (ALS). Its Apache Spark version can run parallelly across a cluster of machines. \n",
        "\n",
        "ALS has some important hyper-parameters to tune during  validation or Cross Validation:\n",
        "\n",
        "- **maxIter**: the maximum number of iterations to run (defaults to 10)\n",
        "\n",
        "- **rank**: the number of latent factors in the model (defaults to 10)\n",
        "\n",
        "- **regParam**: the regularization parameter in ALS (defaults to 1.0)\n",
        "\n",
        "The following code show how to tune these hyperparameters in order to produce the best model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D93bmqzjAuDc",
        "outputId": "9a4e917c-9299-4951-9713-7296927ba23e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC4GOvSBAwaf"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "\n",
        "def tune_ALS(train_data, validation_data, maxIter, regParams, ranks):\n",
        "    \"\"\"\n",
        "    grid search function to select the best model based on RMSE of\n",
        "    validation data\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_data: spark DF with columns ['userId', 'movieId', 'rating']\n",
        "    \n",
        "    validation_data: spark DF with columns ['userId', 'movieId', 'rating']\n",
        "    \n",
        "    maxIter: int, max number of learning iterations\n",
        "    \n",
        "    regParams: list of float, one dimension of hyper-param tuning grid\n",
        "    \n",
        "    ranks: list of float, one dimension of hyper-param tuning grid\n",
        "    \n",
        "    Return\n",
        "    ------\n",
        "    The best fitted ALS model with lowest RMSE score on validation data\n",
        "    \"\"\"\n",
        "    # initial\n",
        "    min_error = float('inf')\n",
        "    best_rank = -1\n",
        "    best_regularization = 0\n",
        "    best_model = None\n",
        "    for rank in ranks:\n",
        "        for reg in regParams:\n",
        "            # get ALS model\n",
        "            als = ALS().setMaxIter(maxIter).setRank(rank).setRegParam(reg)\n",
        "            # train ALS model\n",
        "            model = als.fit(train_data)\n",
        "            # evaluate the model by computing the RMSE on the validation data\n",
        "            predictions = model.transform(validation_data)\n",
        "            evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
        "                                            labelCol=\"rating\",\n",
        "                                            predictionCol=\"prediction\")\n",
        "            rmse = evaluator.evaluate(predictions)\n",
        "            print('{} latent factors and regularization = {}: '\n",
        "                  'validation RMSE is {}'.format(rank, reg, rmse))\n",
        "            if rmse < min_error:\n",
        "                min_error = rmse\n",
        "                best_rank = rank\n",
        "                best_regularization = reg\n",
        "                best_model = model\n",
        "    print('\\nThe best model has {} latent factors and '\n",
        "          'regularization = {}'.format(best_rank, best_regularization))\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvz6kBhXAzi8"
      },
      "source": [
        "**Q: How does matrix factorization improve our recommendations (compared to, for example, KNNs)?**\n",
        "1. Model learns to factor rating matrix into user-matrix and item-matrix, allowing us to predict personalized item rating for each user.\n",
        "\n",
        "1. Less-popular items can have rich latent representations as much as more-popular items, improving the recommender's ability to recommend less-popular items.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EiwEZVjEX8u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# spark imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.mllib.recommendation import ALS\n",
        "\n",
        "# data science imports\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# visualization imports\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuDEiI7kEaRV"
      },
      "outputs": [],
      "source": [
        "# spark config\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"movie recommendation\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"96g\") \\\n",
        "    .config(\"spark.driver.memory\", \"96g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.master\", \"local[12]\") \\\n",
        "    .getOrCreate()\n",
        "# get spark context\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBLN6cd_E0TC"
      },
      "source": [
        "## Project Content\n",
        "1. Load Data\n",
        "2. Spark SQL and OLAP\n",
        "3. Spark ALS-based approach for training model\n",
        "4. ALS Model Selection and Evaluation\n",
        "5. Model testing\n",
        "6. Make movie recommendations to me\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSCKSwEUFFb2"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX5viKD3FDff"
      },
      "outputs": [],
      "source": [
        "i_file = 'movies.csv' # item file\n",
        "iu_file = 'ratings.csv' #item user interaction file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SVnZURuE9gW"
      },
      "outputs": [],
      "source": [
        "# name the path as per your folder's structure and nomenclature\n",
        "items = spark.read.load('drive/MyDrive/amazing_recommender/movies.csv', format='csv', header=True, inferSchema=True)\n",
        "iu = spark.read.load('drive/MyDrive/amazing_recommender/ratings.csv' , format='csv', header=True, inferSchema=True)\n",
        "links = spark.read.load('drive/MyDrive/amazing_recommender/links.csv', format='csv', header=True, inferSchema=True)\n",
        "tags = spark.read.load('drive/MyDrive/amazing_recommender/tags.csv', format='csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wif-oq14FOiQ"
      },
      "outputs": [],
      "source": [
        "# to de-couple code and data, define some standarized variables\n",
        "u_id = 'userId'\n",
        "i_id = 'movieId'\n",
        "rating = 'rating'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnEa202AHMZf"
      },
      "source": [
        "### Basic inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy_sqUzoHP-V",
        "outputId": "c4dec8e2-822b-40a4-a5bd-1e282637d1cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------------+--------------------+\n",
            "|movieId|               title|              genres|\n",
            "+-------+--------------------+--------------------+\n",
            "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
            "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
            "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
            "+-------+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "items.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBRnXfADHWcj",
        "outputId": "36db9600-394d-422d-a8ab-726d7008eb84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+------+---------+\n",
            "|userId|movieId|rating|timestamp|\n",
            "+------+-------+------+---------+\n",
            "|     1|      1|   4.0|964982703|\n",
            "|     1|      3|   4.0|964981247|\n",
            "|     1|      6|   4.0|964982224|\n",
            "+------+-------+------+---------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "iu.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-VGfBsUHYdQ",
        "outputId": "e92be8c9-4b5f-41f1-e262-c978f9052c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------+------+\n",
            "|movieId|imdbId|tmdbId|\n",
            "+-------+------+------+\n",
            "|      1|114709|   862|\n",
            "|      2|113497|  8844|\n",
            "|      3|113228| 15602|\n",
            "+-------+------+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "links.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKn6n_e5HbWZ",
        "outputId": "65eaeb18-d41a-4359-f812-0c927c55b59b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+---------------+----------+\n",
            "|userId|movieId|            tag| timestamp|\n",
            "+------+-------+---------------+----------+\n",
            "|     2|  60756|          funny|1445714994|\n",
            "|     2|  60756|Highly quotable|1445714996|\n",
            "|     2|  60756|   will ferrell|1445714992|\n",
            "+------+-------+---------------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tags.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAAnwuUAHhb-"
      },
      "source": [
        "## Spark SQL and OLAP\n",
        "\n",
        "Below are the EDA questions we would like to ask:\n",
        "1. What are the ratings?\n",
        "1. What is the minimum number of ratings per user and the minimum number of ratings per movie?\n",
        "1. How many movies are rated by only one user?\n",
        "1. What is the total number of users in the data set?\n",
        "1. What is the total number of movies in the data set?\n",
        "1. How many movies are rated by users? List the movies not rated yet.\n",
        "1. List all movie genres\n",
        "1. Find out the number of movies for each category\n",
        "1. Calculate the total rating count for every movie\n",
        "1. Get a count plot for each rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcFLRqNUHojF"
      },
      "source": [
        "What are the ratings?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RLZTR_fHraI",
        "outputId": "efbce11c-33bc-498f-cb22-1a78570cd61a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distinct values of ratings:\n",
            "[0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n"
          ]
        }
      ],
      "source": [
        "print('Distinct values of ratings:')\n",
        "print(sorted(iu.select(rating).distinct().rdd.map(lambda r: r[0]).collect()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rab3FozmHvKs"
      },
      "source": [
        "What are the minimum number of ratings per user and the minimum number of ratings per movie?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQhV41K3Hxx2",
        "outputId": "2e7c93a0-77c0-4701-e2d7-22d8c4dae080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For the users that rated movies and the movies that were rated:\n",
            "Minimum number of ratings per user is 20\n",
            "Minimum number of ratings per movie is 1\n"
          ]
        }
      ],
      "source": [
        "tmp1 = iu.groupBy(u_id).count().toPandas()['count'].min()\n",
        "tmp2 = iu.groupBy(i_id).count().toPandas()['count'].min()\n",
        "print('For the users that rated movies and the movies that were rated:')\n",
        "print('Minimum number of ratings per user is {}'.format(tmp1))\n",
        "print('Minimum number of ratings per movie is {}'.format(tmp2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYya9HkIH2h4"
      },
      "source": [
        "How many movies are rated by only one user?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yD1WO8hH4Xu",
        "outputId": "a7dfee28-979c-4261-b177-82f8ead4efe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3446 out of 9724 movies are rated by only one user\n"
          ]
        }
      ],
      "source": [
        "tmp1 = sum(iu.groupBy(i_id).count().toPandas()['count'] == 1)\n",
        "tmp2 = iu.select(i_id).distinct().count()\n",
        "print('{} out of {} movies are rated by only one user'.format(tmp1, tmp2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO4XjjDTH7Fd"
      },
      "source": [
        "What is the total number of users in the data sets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEtnMxyeH9cv",
        "outputId": "a995e66c-e6fb-4676-a2c5-2f96ffd33110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have a total of 9724 distinct users in the data sets\n"
          ]
        }
      ],
      "source": [
        "tmp = iu.select(i_id).distinct().count()\n",
        "print('We have a total of {} distinct users in the data sets'.format(tmp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erbrVXKbIANQ"
      },
      "source": [
        "What is the total number of movies in the data sets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVMGCrGLIDRI",
        "outputId": "f7fa9b17-d3d7-49ed-9fd4-3f93bb6f9e33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have a total of 9742 distinct movies in the data sets\n"
          ]
        }
      ],
      "source": [
        "tmp = items.select('movieID').distinct().count()\n",
        "print('We have a total of {} distinct movies in the data sets'.format(tmp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Lg3F06IHIT"
      },
      "source": [
        "How many movies are rated by users? List movies not rated yet?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDUwrjy0IKZV",
        "outputId": "67bbb08a-4513-473e-aa97-e321354dbd1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have a total of 9724 distinct movies that are rated by users in ratings table\n",
            "We have 18 movies that are not rated yet\n"
          ]
        }
      ],
      "source": [
        "tmp1 = items.select(i_id).distinct().count()\n",
        "tmp2 = iu.select(i_id).distinct().count()\n",
        "print('We have a total of {} distinct movies that are rated by users in ratings table'.format(tmp2))\n",
        "print('We have {} movies that are not rated yet'.format(tmp1-tmp2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZAcKBG6INDO",
        "outputId": "27743ed4-e1cd-4dfd-8e7d-43aab963ed47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List movies that are not rated yet: \n",
            "+-------+--------------------+\n",
            "|movieId|               title|\n",
            "+-------+--------------------+\n",
            "|   1076|Innocents, The (1...|\n",
            "|   2939|      Niagara (1953)|\n",
            "|   3338|For All Mankind (...|\n",
            "|   3456|Color of Paradise...|\n",
            "|   4194|I Know Where I'm ...|\n",
            "|   5721|  Chosen, The (1981)|\n",
            "|   6668|Road Home, The (W...|\n",
            "|   6849|      Scrooge (1970)|\n",
            "|   7020|        Proof (1991)|\n",
            "|   7792|Parallax View, Th...|\n",
            "+-------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create a temp SQL table view for easier query\n",
        "items.createOrReplaceTempView(\"movies\")\n",
        "iu.createOrReplaceTempView(\"ratings\")\n",
        "print('List movies that are not rated yet: ')\n",
        "# SQL query (NOTE: WHERE ... NOT IN ... == ... LEFT JOIN ... WHERE ... IS NULL)\n",
        "# Approach 1\n",
        "spark.sql(\n",
        "    \"SELECT movieId, title \"\n",
        "    \"FROM movies \"\n",
        "    \"WHERE movieId NOT IN (SELECT distinct(movieId) FROM ratings)\"\n",
        ").show(10)\n",
        "# Approach 2\n",
        "# spark.sql(\n",
        "#     \"SELECT m.movieId, m.title \"\n",
        "#     \"FROM movies m LEFT JOIN ratings r ON m.movieId=r.movieId \"\n",
        "#     \"WHERE r.movieId IS NULL\"\n",
        "# ).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW7padocIQPJ"
      },
      "source": [
        "List all movie genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQa6fuSLISYI",
        "outputId": "62f1fd94-8c86-450f-e509-84174dd3585f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All distinct genres: \n",
            "+------------------+\n",
            "|            genres|\n",
            "+------------------+\n",
            "|             Crime|\n",
            "|           Romance|\n",
            "|          Thriller|\n",
            "|         Adventure|\n",
            "|             Drama|\n",
            "|               War|\n",
            "|       Documentary|\n",
            "|           Fantasy|\n",
            "|           Mystery|\n",
            "|           Musical|\n",
            "|         Animation|\n",
            "|         Film-Noir|\n",
            "|(no genres listed)|\n",
            "|              IMAX|\n",
            "|            Horror|\n",
            "|           Western|\n",
            "|            Comedy|\n",
            "|          Children|\n",
            "|            Action|\n",
            "|            Sci-Fi|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# define a udf for splitting the genres string\n",
        "splitter = UserDefinedFunction(lambda x: x.split('|'), ArrayType(StringType()))\n",
        "# query\n",
        "print('All distinct genres: ')\n",
        "items.select(explode(splitter(\"genres\")).alias(\"genres\")).distinct().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgQpsASlIUfO"
      },
      "source": [
        "Find out the number of movies for each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83WE9IurIW6-",
        "outputId": "c2f967ea-46de-4f17-89f2-38bf1a70baae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counts of movies per genre\n",
            "+------------------+-----+\n",
            "|            genres|count|\n",
            "+------------------+-----+\n",
            "|             Drama| 4361|\n",
            "|            Comedy| 3756|\n",
            "|          Thriller| 1894|\n",
            "|            Action| 1828|\n",
            "|           Romance| 1596|\n",
            "|         Adventure| 1263|\n",
            "|             Crime| 1199|\n",
            "|            Sci-Fi|  980|\n",
            "|            Horror|  978|\n",
            "|           Fantasy|  779|\n",
            "|          Children|  664|\n",
            "|         Animation|  611|\n",
            "|           Mystery|  573|\n",
            "|       Documentary|  440|\n",
            "|               War|  382|\n",
            "|           Musical|  334|\n",
            "|           Western|  167|\n",
            "|              IMAX|  158|\n",
            "|         Film-Noir|   87|\n",
            "|(no genres listed)|   34|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Counts of movies per genre')\n",
        "items.select(i_id, explode(splitter(\"genres\")).alias(\"genres\")) \\\n",
        "    .groupby('genres') \\\n",
        "    .count() \\\n",
        "    .sort(desc('count')) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PvUGG9IIZvI"
      },
      "source": [
        "## Spark ALS-based approach for training model\n",
        "1. Reload data\n",
        "2. Split data into train, validation, test\n",
        "3. ALS model selection and evaluation\n",
        "4. Model testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPZ_NRHPIcia"
      },
      "source": [
        "### Reload data\n",
        "We will use an RDD-based API from pyspark.mllib to predict the ratings, so let's reload \"ratings.csv\" using sc.textFile and then convert it to the form of (user, item, rating) tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQsEVkFPIfl5",
        "outputId": "402425ff-7723-4c0c-80ca-c77fb597a081"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, 1, 4.0), (1, 3, 4.0), (1, 6, 4.0)]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load data\n",
        "item_rating = sc.textFile('drive/MyDrive/amazing_recommender/ratings.csv')\n",
        "# preprocess data -- only need [\"userId\", \"movieId\", \"rating\"]\n",
        "header = item_rating.take(1)[0]\n",
        "rating_data = item_rating \\\n",
        "    .filter(lambda line: line!=header) \\\n",
        "    .map(lambda line: line.split(\",\")) \\\n",
        "    .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))) \\\n",
        "    .cache()\n",
        "# check three rows\n",
        "rating_data.take(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghTxj-6EIwkR"
      },
      "source": [
        "\n",
        "### Split data\n",
        "We split the data into training/validation/testing sets using a 6/2/2 ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BLm2ewPI09Y",
        "outputId": "6c883ce4-38ed-4196-dfc4-112db562dbb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PythonRDD[1493] at RDD at PythonRDD.scala:53"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train, validation, test = rating_data.randomSplit([6, 2, 2], seed=99)\n",
        "# cache data\n",
        "train.cache()\n",
        "validation.cache()\n",
        "test.cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHtNgnIXI8q8"
      },
      "source": [
        "### ALS model selection and evaluation\n",
        "The ALS model can use a grid search to find the optimal hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj_I75sUJBmp"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def train_ALS(train_data, validation_data, num_iters, reg_param, ranks):\n",
        "    \"\"\"\n",
        "    Grid Search Function to select the best model based on RMSE of hold-out data\n",
        "    \"\"\"\n",
        "    # initial\n",
        "    min_error = float('inf')\n",
        "    best_rank = -1\n",
        "    best_regularization = 0\n",
        "    best_model = None\n",
        "    for rank in ranks:\n",
        "        for reg in reg_param:\n",
        "            # train ALS model\n",
        "            model = ALS.train(\n",
        "                ratings=train_data,    # (userID, productID, rating) tuple\n",
        "                iterations=num_iters,\n",
        "                rank=rank,\n",
        "                lambda_=reg,           # regularization param\n",
        "                seed=99)\n",
        "            # make prediction\n",
        "            valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
        "            predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "            # get the rating result\n",
        "            ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
        "            # get the RMSE\n",
        "            MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
        "            error = math.sqrt(MSE)\n",
        "            print('{} latent factors and regularization = {}: validation RMSE is {}'.format(rank, reg, error))\n",
        "            if error < min_error:\n",
        "                min_error = error\n",
        "                best_rank = rank\n",
        "                best_regularization = reg\n",
        "                best_model = model\n",
        "    print('\\nThe best model has {} latent factors and regularization = {}'.format(best_rank, best_regularization))\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI6Uc_L5JDyo",
        "outputId": "e8d6e1fd-2904-4dbe-bf17-c1ec10c4024c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16 latent factors and regularization = 0.01: validation RMSE is 1.3117787273189874\n",
            "16 latent factors and regularization = 0.05: validation RMSE is 1.0099134143019566\n",
            "16 latent factors and regularization = 0.1: validation RMSE is 0.9130907778579515\n",
            "18 latent factors and regularization = 0.01: validation RMSE is 1.3154712703199365\n"
          ]
        }
      ],
      "source": [
        "# hyper-param config\n",
        "num_iterations = 10\n",
        "ranks = [16, 18, 20, 22]\n",
        "reg_params = [0.01, 0.05, 0.1]\n",
        "\n",
        "# grid search and select best model\n",
        "start_time = time.time()\n",
        "final_model = train_ALS(train, validation, num_iterations, reg_params, ranks)\n",
        "\n",
        "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgz5-EfRJOXt"
      },
      "source": [
        "### ALS model learning curve\n",
        "As we increase the number of iterations in training ALS, we can see how RMSE changes and whether or not the model is overfitted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3fW23nMJKDn"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def plot_learning_curve(arr_iters, train_data, validation_data, reg, rank):\n",
        "    \"\"\"\n",
        "    Plot function to show learning curve of ALS\n",
        "    \"\"\"\n",
        "    errors = []\n",
        "    for num_iters in arr_iters:\n",
        "        # train ALS model\n",
        "        model = ALS.train(\n",
        "            ratings=train_data,    # (userID, productID, rating) tuple\n",
        "            iterations=num_iters,\n",
        "            rank=rank,\n",
        "            lambda_=reg,           # regularization param\n",
        "            seed=99)\n",
        "        # make prediction\n",
        "        valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
        "        predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "        # get the rating result\n",
        "        ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
        "        # get the RMSE\n",
        "        MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
        "        error = math.sqrt(MSE)\n",
        "        # add to errors\n",
        "        errors.append(error)\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(arr_iters, errors)\n",
        "    plt.xlabel('number of iterations')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('ALS Learning Curve')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGklxXk_JWYN"
      },
      "outputs": [],
      "source": [
        "# create an array of num_iters\n",
        "iter_array = list(range(1, 11))\n",
        "# create learning curve plot\n",
        "plot_learning_curve(iter_array, train, validation, 0.05, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8iis-CJY7D"
      },
      "source": [
        "After three iterations, alternating gradient descent starts to converge at an error of around 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioZ0X6HgJbdn"
      },
      "source": [
        "### Model testing\n",
        "And finally, make a prediction and check the testing error using out-of-sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBEord6SJumK"
      },
      "outputs": [],
      "source": [
        "# make prediction using test data\n",
        "test_data = test.map(lambda p: (p[0], p[1]))\n",
        "predictions = final_model.predictAll(test_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "# get the rating result\n",
        "rates_preds = test.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
        "# get the RMSE\n",
        "MSE = rates_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
        "error = math.sqrt(MSE)\n",
        "print('The out-of-sample RMSE of rating predictions is', round(error, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXRKjt4EMyIh"
      },
      "source": [
        "### Make item recommendation\n",
        "We need to define a function that takes a new user's item rating and outputs the top 10 recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GYM8TCyM1kn"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def get_i_id(df_i, fav_item_list):\n",
        "    \"\"\"\n",
        "    return all i_id (movieId) of user's favorite items (movies)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    df_i: spark Dataframe, items (movies) data\n",
        "    \n",
        "    fav_item_list: list, e.g. user's list of favorite movies\n",
        "    \n",
        "    Return\n",
        "    ------\n",
        "    i_id_list: list of iterm id (e.g. movieId)\n",
        "    \"\"\"\n",
        "    i_id_list = []\n",
        "    for item in fav_item_list:\n",
        "        i_ids = df_i \\\n",
        "            .filter(items.title.like('%{}%'.format(item))) \\\n",
        "            .select(i_id) \\\n",
        "            .rdd \\\n",
        "            .map(lambda r: r[0]) \\\n",
        "            .collect()\n",
        "        i_id_list.extend(i_id)\n",
        "    return list(set(i_id_list))\n",
        "\n",
        "\n",
        "def add_new_user_to_data(train_data, i_id_list, spark_context):\n",
        "    \"\"\"\n",
        "    add new rows with new user, user's movie and ratings to\n",
        "    existing train data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_data: spark RDD, ratings data\n",
        "    \n",
        "    i_id_list: list, list of item id (e.g. movieId(s))\n",
        "\n",
        "    spark_context: Spark Context object\n",
        "    \n",
        "    Return\n",
        "    ------\n",
        "    new train data with the new user's rows\n",
        "    \"\"\"\n",
        "    # get new user id\n",
        "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
        "    # get max rating\n",
        "    max_rating = train_data.map(lambda r: r[2]).max()\n",
        "    # create new user rdd\n",
        "    user_rows = [(new_id, i_id, max_rating) for i_id in i_id_list]\n",
        "    new_rdd = spark_context.parallelize(user_rows)\n",
        "    # return new train data\n",
        "    return train_data.union(new_rdd)\n",
        "\n",
        "\n",
        "def get_inference_data(train_data, df_i, i_id_list):\n",
        "    \"\"\"\n",
        "    return a rdd with the userid and all movies (except ones in i_id_list)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_data: spark RDD, ratings data\n",
        "\n",
        "    df_i: spark Dataframe, e.g. movies data\n",
        "    \n",
        "    i_id_list: list, list of i_id e.g. movieId(s)\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    inference data: Spark RDD\n",
        "    \"\"\"\n",
        "    # get new user id\n",
        "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
        "    # return inference rdd\n",
        "    return df_i.rdd \\\n",
        "        .map(lambda r: r[0]) \\\n",
        "        .distinct() \\\n",
        "        .filter(lambda x: x not in i_id_list) \\\n",
        "        .map(lambda x: (new_id, x))\n",
        "\n",
        "\n",
        "def make_recommendation(best_model_params, ratings_data, df_i, \n",
        "                        fav_item_list, n_recommendations, spark_context):\n",
        "    \"\"\"\n",
        "    return top n item recommendation based on user's input list of favorite items\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    best_model_params: dict, {'iterations': iter, 'rank': rank, 'lambda_': reg}\n",
        "\n",
        "    ratings_data: spark RDD, ratings data\n",
        "\n",
        "    df_i: spark Dataframe, movies data\n",
        "\n",
        "    fav_item_list: list, user's list of favorite items e.g. movies\n",
        "\n",
        "    n_recommendations: int, top n recommendations\n",
        "\n",
        "    spark_context: Spark Context object\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    list of top n item recommendations\n",
        "    \"\"\"\n",
        "    # modify train data by adding new user's rows\n",
        "    i_id_list = get_i_id(df_i, fav_item_list)\n",
        "    train_data = add_new_user_to_data(ratings_data, i_id_list, spark_context)\n",
        "    \n",
        "    # train best ALS\n",
        "    model = ALS.train(\n",
        "        ratings=train_data,\n",
        "        iterations=best_model_params.get('iterations', None),\n",
        "        rank=best_model_params.get('rank', None),\n",
        "        lambda_=best_model_params.get('lambda_', None),\n",
        "        seed=99)\n",
        "    \n",
        "    # get inference rdd\n",
        "    inference_rdd = get_inference_data(ratings_data, df_i, i_id_list)\n",
        "    \n",
        "    # inference\n",
        "    predictions = model.predictAll(inference_rdd).map(lambda r: (r[1], r[2]))\n",
        "    \n",
        "    # get top n movieId\n",
        "    topn_rows = predictions.sortBy(lambda r: r[1], ascending=False).take(n_recommendations)\n",
        "    topn_ids = [r[0] for r in topn_rows]\n",
        "    \n",
        "    # return movie titles\n",
        "    return df_i.filter(items[i_id].isin(topn_ids)) \\\n",
        "                    .select('title') \\\n",
        "                    .rdd \\\n",
        "                    .map(lambda r: r[0]) \\\n",
        "                    .collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmNEpOGyM3id"
      },
      "source": [
        "Let's pretend I am a new user of this recommender system. I will input a handful of my all-time favorite movies into the system. And then, the system should output top N movie recommendations for me to watch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPo55f2DM5b6"
      },
      "outputs": [],
      "source": [
        "# my favorite movies\n",
        "my_favorite_items = ['Iron Man']\n",
        "\n",
        "# get recommends\n",
        "recommends = make_recommendation(\n",
        "    best_model_params={'iterations': 10, 'rank': 20, 'lambda_': 0.05}, \n",
        "    ratings_data=rating_data, \n",
        "    df_i=items, \n",
        "    fav_movie_list=my_favorite_items, \n",
        "    n_recommendations=10, \n",
        "    spark_context=sc)\n",
        "\n",
        "print('Recommendations for {}:'.format(my_favorite_items[0]))\n",
        "for i, title in enumerate(recommends):\n",
        "    print('{0}: {1}'.format(i+1, title))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twvglOaIM8uo"
      },
      "source": [
        "\n",
        "**Comparison with KNN Model-Based Recommendations**\n",
        "\n",
        "This list of movie recommendations looks completely different than the list from my previous **KNN** model recommender.\n",
        "\n",
        "**Exploratory Recommendations**\n",
        "\n",
        "Not only does it recommend movies outside of the years between 2007 and 2009, but it also recommends less known movies. So this can offer users some element of surprise, and users won't get bored by getting the same popular movies all the time.\n",
        "\n",
        "**Combining Recommendations**\n",
        "\n",
        "To achieve a good mix, this list of recommendations can be blended into the previous list of recommendations from the **KNN** model recommender."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
